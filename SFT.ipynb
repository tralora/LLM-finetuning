{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caaa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b71568",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ee589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_gpu=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    if use_gpu:\n",
    "        model = model.to('cuda')\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ee49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, user_message, system_prompt=None, max_new_tokens=100):\n",
    "    messages = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    \n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions, system_prompt=None, title=\"Model Output\"):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = generate_response(model, tokenizer, question, system_prompt)\n",
    "        print(f\"Question {i}: {question}\")\n",
    "        print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35352a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 2 + 2 - 2.\",\n",
    "    \"What's the difference between thread and process?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eea5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"Qwen/Qwen3-0.6B-Base\", use_gpu=True)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, title=\"Qwen3-0.6B-Base (Before SFT) OUTPUT\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"banghua/Qwen3-0.6B-SFT\", use_gpu=True)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbcc850",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/trl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cf0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl\n",
    "print(trl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e342d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B-Base\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70222c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset(dataset):\n",
    "    # Visualize the dataset \n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        example = dataset[i]\n",
    "        user_msg = next(m['content'] for m in example['messages']\n",
    "                        if m['role'] == 'user')\n",
    "        assistant_msg = next(m['content'] for m in example['messages']\n",
    "                             if m['role'] == 'assistant')\n",
    "        rows.append({\n",
    "            'User Prompt': user_msg,\n",
    "            'Assistant Response': assistant_msg\n",
    "        })\n",
    "    \n",
    "    # Display as table\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_colwidth', None)  # Avoid truncating long strings\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3baba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTTrainer config\n",
    "sft_config = SFTConfig(\n",
    "    learning_rate=8e-5, # Learning rate for training\n",
    "    num_train_epochs=1, # Set the number of epochs to train the model\n",
    "    per_device_eval_batch_size=1, # Batch size for each device (e.g., GPU) during training\n",
    "    gradient_accumulation_steps=8, # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    logging_steps=2, # Number of steps between logging events\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTTrainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd471b4",
   "metadata": {},
   "source": [
    "## Upload to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4830e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_dir = \"Qwen3-0.6B-SFT\"\n",
    "model.save_pretrained(path_to_save_dir)\n",
    "tokenizer.save_pretrained(path_to_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Replace with your model repo name\n",
    "repo_name = \"tralora/Qwen3-0.6B-SFT\"\n",
    "save_path = \"Qwen3-0.6B-SFT\"\n",
    "\n",
    "# Upload model\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13380f8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
